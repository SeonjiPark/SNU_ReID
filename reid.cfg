[detect_config]
# Detect_yolo
stride = 32
num_workers = 4
max_det = 1000
batch_size = 1

[reid_config]
### ReID_CTL
backbone_name = resnet50
backbone_path = ./weights/resnet50-19c8e357.pth
normalize_features = True

# Name of backbone
MODEL_NAME = "resnet50"
# Size of embeddings from backbone
MODEL_BACKBONE_EMB_SIZE = 2048
# Last stride of backbone
MODEL_LAST_STRIDE = 1
# Use ImageNet pretrained model to initialize backbone or use 'self' trained
# model to initialize the whole model
# Options: True | False
MODEL_PRETRAINED = True
# Path to weights to load
MODEL_PRETRAIN_PATH = ""
# Create centroids
MODEL_USE_CENTROIDS = False
# Ensures images to build centroids during retrieval
# do not come from the same camera as the query
MODEL_KEEP_CAMID_CENTROIDS = True
# Set True if Pre-traing path points to previously trained/aborted model
MODEL_RESUME_TRAINING = False

# -----------------------------------------------------------------------------
# INPUT
# -----------------------------------------------------------------------------
# Size of the image during training
INPUT_SIZE_TRAIN = [256, 128]
# Size of the image during test
INPUT_SIZE_TEST = [256, 128]
# Random probability for image horizontal flip
INPUT_PROB = 0.5
# Random probability for random erasing
INPUT_RE_PROB = 0.5
# Values to be used for image normalization
INPUT_PIXEL_MEAN = [0.485, 0.456, 0.406]
# Values to be used for image normalization
INPUT_PIXEL_STD = [0.229, 0.224, 0.225]
# Value of padding size
INPUT_PADDING = 10

# -----------------------------------------------------------------------------
# DATASET
# -----------------------------------------------------------------------------
# List of the dataset names for training, as present in paths_catalog.py
# DATASETS_NAMES = "PRW"
# Root directory where datasets should be used (and downloaded if not found)
# DATASETS_ROOT_DIR = "../DATASET"
# Path to json train file for datasets that require it
DATASETS_JSON_TRAIN_PATH = ""

# -----------------------------------------------------------------------------
# DATALOADER
# -----------------------------------------------------------------------------
# Number of data loading threads
DATALOADER_NUM_WORKERS = 6
# Sampler for data loading
DATALOADER_SAMPLER = "random_identity"
# Number of instance for one batch
DATALOADER_NUM_INSTANCE = 4
# Whether to drop last not full batch
DATALOADER_DROP_LAST = True
# Whether to use resampling in case when number of samples < DATALOADER.NUM_INSTANCE:
# True for Baseline_ False for CTLModel
DATALOADER_USE_RESAMPLING = True

# ---------------------------------------------------------------------------- #
# SOLVER
# ---------------------------------------------------------------------------- #
# Name of optimizer
SOLVER_OPTIMIZER_NAME = "Adam"
# Number of max epoches
SOLVER_MAX_EPOCHS = 120
# Base learning rate
SOLVER_BASE_LR = 1e-4
# Momentum
SOLVER_MOMENTUM = 0.9
# Margin of triplet loss
SOLVER_MARGIN = 0.5
# Function used to compute distance (euclidean or cosine for now)
SOLVER_DISTANCE_FUNC = "euclidean"
# # Margin of cluster
SOLVER_CLUSTER_MARGIN = 0.3
# # Learning rate of SGD to learn the centers of center loss
SOLVER_CENTER_LR = 0.5
# # Balanced weight of center loss
SOLVER_CENTER_LOSS_WEIGHT = 0.0005
# Settings of weight decay
SOLVER_WEIGHT_DECAY = 0.0005
SOLVER_WEIGHT_DECAY_BIAS = 0.0005
# name of LR scheduler
SOLVER_LR_SCHEDULER_NAME = "multistep_lr"
# decay rate of learning rate
SOLVER_GAMMA = 0.1
# decay step of learning rate
SOLVER_LR_STEPS = (40, 70)
# warm up factor
SOLVER_USE_WARMUP_LR = True
# epochs of warm up
SOLVER_WARMUP_EPOCHS = 10
# Metric name for checkpointing best model
SOLVER_MONITOR_METRIC_NAME = "mAP"
# Metric value mode used for checkpointing (max, min, auto)
SOLVER_MONITOR_METRIC_MODE = "max"
# epoch number of saving checkpoints
SOLVER_CHECKPOINT_PERIOD = 50
# epoch number of validation
SOLVER_EVAL_PERIOD = 5
# Number of images per batch PER GPU
SOLVER_IMS_PER_BATCH = 64
# 'dp', 'ddp', 'ddp2', 'ddp_spawn' - see pytorch lighning options
SOLVER_DIST_BACKEND = "ddp"
# Losses weights
# Weight of classification loss on query vectors
SOLVER_QUERY_XENT_WEIGHT = 1.0
# Weight of contrastive loss on query vectors
SOLVER_QUERY_CONTRASTIVE_WEIGHT = 1.0
# Weight of contrastive loss on centroids-query vectors
SOLVER_CENTROID_CONTRASTIVE_WEIGHT = 1.0
# Whether to use automatic Python Lightning optimization
SOLVER_USE_AUTOMATIC_OPTIM = False

# ---------------------------------------------------------------------------- #
# TEST
# ---------------------------------------------------------------------------- #
# Number of images per batch during test
TEST_IMS_PER_BATCH = 128
# Path to trained model
TEST_WEIGHT = ""
# # Whether feature is nomalized before test, if yes, it is equivalent to cosine distance
TEST_FEAT_NORM = True
# Only run test
TEST_ONLY_TEST = False
# If to visualize rank results
TEST_VISUALIZE = "no"
# What top-k results to rank
TEST_VISUALIZE_TOPK = 10
# Max number of query images plotted
TEST_VISUALIZE_MAX_NUMBER = 1000000

# ---------------------------------------------------------------------------- #
# MISC
# ---------------------------------------------------------------------------- #
# Log root directory
LOG_DIR = "logs"
# Whether to use mixed precision
USE_MIXED_PRECISION = True

# ---------------------------------------------------------------------------- #
# REPORDUCIBLE EXPERIMENTS
# ---------------------------------------------------------------------------- #
# Whether to seed everything
REPRODUCIBLE = False
# Number of runs with seeded generators
REPRODUCIBLE_NUM_RUNS = 3
# Seed to start with
REPRODUCIBLE_SEED = 0